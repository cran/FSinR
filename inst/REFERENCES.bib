@article{Arauzo2007,
  title         = {Consistency measures for feature selection},
  author        = {Arauzo-Azofra, Antonio and Benitez, Jose Manuel and Castro, Juan Luis},
  year          = 2007,
  month         = feb,
  journal       = {Journal of Intelligent Information Systems},
  publisher     = {Springer Nature},
  volume        = 30,
  number        = 3,
  pages         = {273–292},
  doi           = {10.1007/s10844-007-0037-0},
  issn          = {1573-7675},
  url           = {http://dx.doi.org/10.1007/s10844-007-0037-0}
}

@article{Narendra1977,
  title         = {A Branch and Bound Algorithm for Feature Subset Selection},
  author        = {P. Narendra and K. Fukunaga},
  year          = 1977,
  month         = sep,
  journal       = {IEEE Transactions on Computers},
  publisher     = {IEEE Computer Society},
  address       = {Los Alamitos, CA, USA},
  volume        = 26,
  number        = 9,
  pages         = {917--922},
  doi           = {10.1109/TC.1977.1674939},
  issn          = {0018-9340},
  keywords      = {branch and bound;combinatorial optimization;feature selection;recursive computation.}
}

@inproceedings{Sheinvald1990,
  title         = {A modeling approach to feature selection},
  author        = {J. {Sheinvald} and B. {Dom} and W. {Niblack}},
  year          = 1990,
  month         = jun,
  booktitle     = {[1990] Proceedings. 10th International Conference on Pattern Recognition},
  volume        = {i},
  number        = {},
  pages         = {535--539 vol.},
  doi           = {10.1109/ICPR.1990.118160},
  issn          = {},
  keywords      = {information theory;pattern recognition;picture processing;probability;information theory;modeling;feature selection;useless feature-subset;probability model;minimum-description-length criterion;Gaussian case;geometric interpretation;Error analysis;Computer vision;Jacobian matrices;Closed-form solution;H infinity control;Hardware;Bayesian methods;Noise measurement;Training data}
}

@article{Arauzo2004,
  title         = {A feature set measure based on Relief},
  author        = {Arauzo-Azofra, Antonio and Ben\'{i}tez, Jos\'{e} and Castro, Juan},
  year          = 2004,
  month         = 1,
  journal       = {Proceedings of the 5th International Conference on Recent Advances in Soft Computing},
  pages         = {}
}

@article{Wang2018,
  title         = {A hybrid system with filter approach and multiple population genetic algorithm for feature selection in credit scoring},
  author        = {Di Wang and Zuoquan Zhang and Rongquan Bai and Yanan Mao},
  year          = 2018,
  journal       = {Journal of Computational and Applied Mathematics},
  volume        = 329,
  pages         = {307--321},
  doi           = {https://doi.org/10.1016/j.cam.2017.04.036},
  issn          = {0377-0427},
  url           = {https://www.sciencedirect.com/science/article/abs/pii/S0377042717302078},
  note          = {The International Conference on Information and Computational Science, 2--6 August 2016, Dalian, China},
  keywords      = {Credit scoring, Feature selection, Hybrid approach, HMPGA},
  abstract      = {With the financial crisis happened in 2007, massive credit risks are exposed to the banking sectors. So credit scoring has attracted more and more attention. Bank owns a lot of customer data. By using those data, credit scoring model can judge the applicants' credit risk accurately. But those data are often high dimensional, and have some irrelevant features. Those irrelevant features will affect classifiers accuracy. Therefore, feature selection is an important topic. This paper proposes a two-phase hybrid approach based on filter approach and multiple population genetic algorithm-HMPGA. In phase 1, it introduces the idea of wrapper approach into three filter approaches to acquire some important prior information for initial populations setting of MPGA. In phase 2, it takes advantage of MPGA's characteristics of global optimization and quick convergence to find optimal feature subset. This paper uses two real credit scoring datasets of UCI databases to compare HMPGA, MPGA and GA. It verifies that the accuracies of feature subsets acquired from HMPGA, MPGA and GA are superior to three filter approaches. Meanwhile, nonparametric Wilcoxon signed rank test is held to confirm that HMPGA is better than MPGA and GA. HMPGA not only can be applied to feature selection of credit scoring, but also can be applied to more fields of data mining.}
}
@article{Pawlak1982,
  title         = {Rough sets},
  author        = {Pawlak, Zdzisław},
  year          = 1982,
  month         = october,
  journal       = {International Journal of Computer \& Information Sciences},
  volume        = 11,
  pages         = {341--356},
  doi           = {https://doi.org/10.1007/BF01001956},
  issn          = {0091-7036},
  url           = {https://www.sciencedirect.com/science/article/abs/pii/S0377042717302078},
  keywords      = {Artificial intelligence, automatic classification, cluster analysis, fuzzy sets, inductive reasoning, learning algorithms, measurement theory, pattern recognition, tolerance theory },
  abstract      = {We investigate in this paper approximate operations on sets, approximate equality of sets, and approximate inclusion of sets. The presented approach may be considered as an alternative to fuzzy sets theory and tolerance theory. Some applications are outlined.}
}
@book{Pawlak1991,
  title         = {Rough sets: Theoretical aspects of reasoning about data},
  author        = {Pawlak, Zdzisław},
  year          = 1991,
  publisher     = {Springer, Dordrecht},
  volume        = 9,
  number        = 1,
  pages         = {XVI–231},
  doi           = {10.1007/978-94-011-3534-4},
  url           = {http://dx.doi.org/10.1007/978-94-011-3534-4}
}
@inproceedings{AlmuallimDietterich1991,
    author = {Almuallim, Hussein and Dietterich, Thomas G.},
    title = {Learning With Many Irrelevant Features},
    booktitle = {In Proceedings of the Ninth National Conference on Artificial Intelligence},
    year = {1991},
    pages = {547--552},
    publisher = {AAAI Press}
}
@article{DashLiu2003,
 author = {Dash, Manoranjan and Liu, Huan},
 title = {Consistency-based Search in Feature Selection},
 journal = {Artif. Intell.},
 issue_date = {December 2003},
 volume = {151},
 number = {1-2},
 month = dec,
 year = {2003},
 issn = {0004-3702},
 pages = {155--176},
 numpages = {22},
 url = {http://dx.doi.org/10.1016/S0004-3702(03)00079-1},
 doi = {10.1016/S0004-3702(03)00079-1},
 acmid = {967525},
 publisher = {Elsevier Science Publishers Ltd.},
 address = {Essex, UK},
 keywords = {branch and bound, classification, evaluation measures, feature selection, random search, search strategies},
}
@article{QianShu2015,
abstract = {Feature selection is an important preprocessing step in machine learning and data mining, and feature criterion arises a key issue in the construction of feature selection algorithms. Mutual information is one of the widely used criteria in feature selection, which determines the relevance between features and target classes. Some mutual information-based feature selection algorithms have been extensively studied, but less effort has been made to investigate the feature selection issue in incomplete data. In this paper, combined with the tolerance information granules in rough sets, the mutual information criterion is provided for evaluating candidate features in incomplete data, which not only utilizes the largest mutual information with the target class but also takes into consideration the redundancy between selected features. We first validate the feasibility of the mutual information. Then an effective mutual information-based feature selection algorithm with forward greedy strategy is developed in incomplete data. To further accelerate the feature selection process, the selection of candidate features is implemented in a dwindling object set. Compared with existing feature selection algorithms, the experimental results on different real data sets show that the proposed algorithm is more effective for feature selection in incomplete data at most cases.},
author = {Qian, Wenbin and Shu, Wenhao},
doi = {10.1016/j.neucom.2015.05.105},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Feature selection,Incomplete data,Mutual information,Rough sets,Uncertainty measure},
pages = {210--220},
publisher = {Elsevier},
title = {{Mutual information criterion for feature selection from incomplete data}},
url = {http://dx.doi.org/10.1016/j.neucom.2015.05.105},
volume = {168},
year = {2015}
}

@article{Whitney1971,
 author = {Whitney, A. W.},
 title = {A Direct Method of Nonparametric Measurement Selection},
 journal = {IEEE Trans. Comput.},
 issue_date = {September 1971},
 volume = {20},
 number = {9},
 month = sep,
 year = {1971},
 issn = {0018-9340},
 pages = {1100--1103},
 numpages = {4},
 url = {http://dx.doi.org/10.1109/T-C.1971.223410},
 doi = {10.1109/T-C.1971.223410},
 acmid = {1311492},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Feature space evaluation, measurement (feature) selection, nearest neighbor rule, pattern classification.},
}

@article{MarillGreen1963,
    annote = {In the type of recognition system under discussion, the physical sample to be recognized is first subjected to a battery of tests; on the basis of the test results, the sample is then assigned to one of a number of prespecified categories. The theory of how test results should be combined to yield an optimal assignment has been discussed in an earlier paper. Here, attention is focused on the tests themselves. At present, we usually measure the effectiveness of a set of tests empirically, i.e., by determining the percentage of correct recognitions made by some recognition device which uses these tests. In this paper, we discuss some of the theoretical problems encountered in trying to determine a more formal measure of the effectiveness of a set of tests; a measure which might be a practical substitute for the empirical evaluation. Specifically, the following question is considered: What constitutes an effective set of tests, and how is this effectiveness dependent on the correlations among, and the properties of, the individual tests in the set? Specific suggestions are considered for the case in which the test results are normally distributed, but arbitrarily correlated. The discussion is supported by the results of experiments dealing with automatic recognition of hand-printed characters.},
    author = {Marill, T. and Green, D.},
    booktitle = {Information Theory, IEEE Transactions on},
    citeulike-article-id = {5947542},
    journal = {Information Theory, IEEE Transactions on},
    keywords = {file-import-09-10-16, timeseries},
    number = {1},
    pages = {11--17},
    priority = {2},
    title = {On the effectiveness of receptors in recognition systems},
    volume = {9},
    month = {02},
    year = {1963},
    url = {http://dx.doi.org/10.1109/TIT.1963.1057810},
doi = {10.1109/TIT.1963.1057810}
}
@article{KirkpatrickGelattVecchi1983,
    author = {S. Kirkpatrick and C. D. Gelatt and M. P. Vecchi},
    title = {Optimization by simulated annealing},
    url = {http://dx.doi.org/10.1126/science.220.4598.671},
    doi = {10.1126/science.220.4598.671 },
    journal = {SCIENCE},
    year = {1983},
    volume = {220},
    number = {4598},
    pages = {671--680}
}
@article{Kumar2018,
author="Kumar, Vijay
and Kumar, Dinesh",
title="Binary whale optimization algorithm and its application to unit commitment problem",
journal="Neural Computing and Applications",
year="2018",
month="Oct",
day="16",
abstract="Whale optimization algorithm is a novel metaheuristic algorithm that imitates the social behavior of humpback whales. In this algorithm, the bubble-net hunting strategy of humpback whales is exploited. However, this algorithm, in its present form, is appropriate for continuous problems. To make it applicable to discrete problems, a binary version of this algorithm is being proposed in this paper. In the proposed approach, the solutions are binarized and sigmoidal transfer function is utilized to update the position of whales. The performance of the proposed algorithm is evaluated on 29 benchmark functions. Furthermore, unpaired t test is carried out to illustrate its statistical significance. The experimental results depict that the proposed algorithm outperforms others in respect of benchmark test functions. The proposed approach is applied on electrical engineering problem, a real-life application, named as ``unit commitment''. The proposed approach uses the priority list to handle spinning reserve constraints and search mechanism to handle minimum up/down time constraints. It is tested on standard IEEE systems consisting of 4, 10, 20, 40, 80, and 100 units and on IEEE 118-bus system and Taiwan 38-bus system as well. Experimental results reveal that the proposed approach is superior to other algorithms in terms of lower production cost.",
issn="1433-3058",
doi="10.1007/s00521-018-3796-3",
url="https://doi.org/10.1007/s00521-018-3796-3"
}
@incollection{yang1998feature,
  title={Feature subset selection using a genetic algorithm},
  author={Yang, Jihoon and Honavar, Vasant},
  booktitle={Feature extraction, construction and selection},
  pages={117--136},
  year={1998},
  publisher={Springer}
}
@Article{GAPkg1,
  title = {{GA}: A Package for Genetic Algorithms in {R}},
  author = {Luca Scrucca},
  journal = {Journal of Statistical Software},
  year = {2013},
  volume = {53},
  number = {4},
  pages = {1--37},
  url = {https://www.jstatsoft.org/article/view/v053i04},
}

@Article{GAPkg2,
  title = {On some extensions to {GA} package: hybrid optimisation,
    parallelisation and islands evolution},
  author = {Luca Scrucca},
  journal = {The R Journal},
  year = {2017},
  volume = {9},
  number = {1},
  pages = {187--206},
  url = {https://journal.r-project.org/articles/RJ-2017-008/},
}

@Manual{FSelectorPkg,
    title = {FSelector: Selecting Attributes},
    author = {Piotr Romanski and Lars Kotthoff},
    year = {2018},
    note = {R package version 0.31},
    url = {https://CRAN.R-project.org/package=FSelector},
  }

@inproceedings{LiuSetiono1996,
    author = {Huan Liu and Rudy Setiono},
    title = {Feature Selection And Classification - A Probabilistic Wrapper Approach},
    booktitle = {in Proceedings of the 9th International Conference on Industrial and Engineering Applications of AI and ES},
    year = {1996},
    pages = {419--424}
}
@article{Pudil1994,
  title={Floating search methods in feature selection},
  author={Pudil, Pavel and Novovi{\v{c}}ov{\'a}, Jana and Kittler, Josef},
  journal={Pattern recognition letters},
  volume={15},
  number={11},
  pages={1119--1125},
  year={1994},
  publisher={Elsevier}
}

@incollection{Kira1992,
  title={A practical approach to feature selection},
  author={Kira, Kenji and Rendell, Larry A},
  booktitle={Machine Learning Proceedings 1992},
  pages={249--256},
  year={1992},
  publisher={Elsevier}
}

@article{Kashef2015,
  title = "An advanced ACO algorithm for feature subset selection",
  journal = "Neurocomputing",
  volume = "147",
  pages = "271 - 279",
  year = "2015",
  note = "Advances in Self-Organizing Maps Subtitle of the special issue: Selected Papers from the Workshop on Self-Organizing Maps 2012 (WSOM 2012)",
  issn = "0925-2312",
  doi = "https://doi.org/10.1016/j.neucom.2014.06.067",
  url = "https://www.sciencedirect.com/science/article/abs/pii/S0925231214008601",
  author = "Shima Kashef and Hossein Nezamabadi-pour",
  keywords = "Feature selection, Wrapper, Ant colony optimization (ACO), Binary ACO, Classification",
}

@article{hsu2011hybrid,
  title={Hybrid feature selection by combining filters and wrappers},
  author={Hsu, Hui-Huang and Hsieh, Cheng-Wei and Lu, Ming-Da},
  journal={Expert Systems with Applications},
  volume={38},
  number={7},
  pages={8144--8150},
  year={2011},
  publisher={Elsevier}
}

@article{hu2015hybrid,
  title={Hybrid filter--wrapper feature selection for short-term load forecasting},
  author={Hu, Zhongyi and Bao, Yukun and Xiong, Tao and Chiong, Raymond},
  journal={Engineering Applications of Artificial Intelligence},
  volume={40},
  pages={17--27},
  year={2015},
  publisher={Elsevier}
}

@Book{ Cramer1946,
  author = { Cramer, Harald, },
  title = { Mathematical methods of statistics / by Harald Cramer },
  publisher = { Princeton University Press Princeton },
  pages = { xvi, 575 p. ; },
  year = { 1946 },
  type = { Book },
  language = { English },
  subjects = { Mathematical statistics. },
  life-dates = { 1946 -  },
  isbn =  {ISBN 0-691-08004-6},
  catalogue-url = { https://nla.gov.au/nla.cat-vn81100 },
}

@article{Pearson1900,
  author = { Karl   Pearson   F.R.S. },
  title = {X. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling},
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {50},
  number = {302},
  pages = {157-175},
  year  = {1900},
  publisher = {Taylor & Francis},
  doi = {10.1080/14786440009463897},
  URL = { https://doi.org/10.1080/14786440009463897 },
  eprint = { https://doi.org/10.1080/14786440009463897 }
}

@Article{Quinlan1986,
author="Quinlan, J. R.",
title="Induction of decision trees",
journal="Machine Learning",
year="1986",
month="Mar",
day="01",
volume="1",
number="1",
pages="81--106",
abstract="The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.",
issn="1573-0565",
doi="10.1007/BF00116251",
url="https://doi.org/10.1007/BF00116251"
}

  @Book{WittenFrank2005,
    title = {Data Mining: Practical Machine Learning Tools and
      Techniques},
    author = {Ian H. Witten and Eibe Frank},
    year = {2005},
    edition = {2nd},
    publisher = {Morgan Kaufmann},
    address = {San Francisco},
  }

@Book{Kozen1992,
author="Kozen, Dexter C.",
title="Depth-First and Breadth-First Search",
bookTitle="The Design and Analysis of Algorithms",
year="1992",
publisher="Springer New York",
address="New York, NY",
pages="19--24",
abstract="Depth-first search (DFS) and breadth-first search (BFS) are two of the most useful subroutines in graph algorithms. They allow one to search a graph in linear time and compile information about the graph. They differ in that the former uses a stack (LIFO) discipline and the latter uses a queue (FIFO) discipline to choose the next edge to explore.",
isbn="978-1-4612-4400-4",
doi="10.1007/978-1-4612-4400-4_4",
url="https://doi.org/10.1007/978-1-4612-4400-4_4"
}

@Book{rsquared,
title="Coefficient of Determination",
author="Yadolah Dodge",
bookTitle="The Concise Encyclopedia of Statistics",
year="2008",
publisher="Springer New York",
address="New York, NY",
pages="88--91",
isbn="978-0-387-32833-1",
doi="10.1007/978-0-387-32833-1_62",
url="https://doi.org/10.1007/978-0-387-32833-1_62"
}

@Article{Ceriani2012,
author="Ceriani, Lidia
and Verme, Paolo",
title="The origins of the Gini index: extracts from Variabilit{\`a} e Mutabilit{\`a} (1912) by Corrado Gini",
journal="The Journal of Economic Inequality",
year="2012",
month="Sep",
day="01",
volume="10",
number="3",
pages="421--443",
abstract="The scope of this paper is to celebrate the 100th anniversary of the Gini index by providing the original formulae. Corrado Gini introduced his index for the first time in a 1912 book published in Italian under the name of ``Variabilit{\`a} e Mutabilit{\`a}'' (Variability and Mutability). This article provides selected extracts of Part I of the book dedicated to measures of variability. We find that Gini proposed no less than 13 formulations of his index, none of which is known today to the large public. We also find that Gini anticipated some of the developments that derived from the study of his index.",
issn="1573-8701",
doi="10.1007/s10888-011-9188-x",
url="https://doi.org/10.1007/s10888-011-9188-x"
}

@article{kohavi1997,
  title={Wrappers for feature subset selection},
  author={Kohavi, Ron and John, George H},
  journal={Artificial intelligence},
  volume={97},
  number={1-2},
  pages={273--324},
  year={1997},
  publisher={Elsevier}
}

@Manual{caret,
    title = {caret: Classification and Regression Training},
    author = {Max Kuhn. Contributions from Jed Wing and Steve Weston and Andre Williams and Chris Keefer and Allan Engelhardt and Tony Cooper and Zachary Mayer and Brenton Kenkel and the R Core Team and Michael Benesty and Reynald Lescarbeau and Andrew Ziem and Luca Scrucca and Yuan Tang and Can Candan and Tyler Hunt.},
    year = {2018},
    note = {R package version 6.0-80},
    url = {https://CRAN.R-project.org/package=caret},
  }
@article{glover1990,
author = {Glover, Fred},
title = {Tabu Search—Part I},
journal = {ORSA Journal on Computing},
volume = {1},
number = {3},
pages = {190-206},
year = {1989},
doi = {10.1287/ijoc.1.3.190},
URL = {https://doi.org/10.1287/ijoc.1.3.190},
eprint = {https://doi.org/10.1287/ijoc.1.3.190},
    abstract = { This paper presents the fundamental principles underlying tabu search as a strategy for combinatorial optimization problems. Tabu search has achieved impressive practical successes in applications ranging from scheduling and computer channel balancing to cluster analysis and space planning, and more recently has demonstrated its value in treating classical problems such as the traveling salesman and graph coloring problems. Nevertheless, the approach is still in its infancy, and a good deal remains to be discovered about its most effective forms of implementation and about the range of problems for which it is best suited. This paper undertakes to present the major ideas and findings to date, and to indicate challenges for future research. Part I of this study indicates the basic principles, ranging from the short-term memory process at the core of the search to the intermediate and long term memory processes for intensifying and diversifying the search. Included are illustrative data structures for implementing the tabu conditions (and associated aspiration criteria) that underlie these processes. Part I concludes with a discussion of probabilistic tabu search and a summary of computational experience for a variety of applications. Part II of this study (to appear in a subsequent issue) examines more advanced considerations, applying the basic ideas to special settings and outlining a dynamic move structure to insure finiteness. Part II also describes tabu search methods for solving mixed integer programming problems and gives a brief summary of additional practical experience, including the use of tabu search to guide other types of processes, such as those of neural networks. INFORMS Journal on Computing, ISSN 1091-9856, was published as ORSA Journal on Computing from 1989 to 1995 under ISSN 0899-1499. }
}
@article{glover1986,
 author = {Glover, Fred},
 title = {Future Paths for Integer Programming and Links to Artificial Intelligence},
 journal = {Comput. Oper. Res.},
 issue_date = {May 1986},
 volume = {13},
 number = {5},
 month = may,
 year = {1986},
 issn = {0305-0548},
 pages = {533--549},
 numpages = {17},
 url = {http://dx.doi.org/10.1016/0305-0548(86)90048-1},
 doi = {10.1016/0305-0548(86)90048-1},
 acmid = {15311},
 publisher = {Elsevier Science Ltd.},
 address = {Oxford, UK, UK},
}

@book{Russell2009,
 author = {Russell, Stuart and Norvig, Peter},
 title = {Artificial Intelligence: A Modern Approach},
 year = {2009},
 isbn = {0136042597, 9780136042594},
 edition = {3rd},
 publisher = {Prentice Hall Press},
 address = {Upper Saddle River, NJ, USA},
}
@InProceedings{ShinXu2009,
author="Shin, Kilho
and Xu, Xian Ming",
editor="Vel{\'a}squez, Juan D.
and R{\'i}os, Sebasti{\'a}n A.
and Howlett, Robert J.
and Jain, Lakhmi C.",
title="Consistency-Based Feature Selection",
booktitle="Knowledge-Based and Intelligent Information and Engineering Systems",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="342--350",
abstract="Feature selection, the job to select features relevant to classification, is a central problem of machine learning. Inconsistency rate is known as an effective measure to evaluate consistency (relevance) of feature subsets, and INTERACT, a state-of-the-art feature selection algorithm, takes advantage of it. In this paper, we shows that inconsistency rate is not the unique measure of consistency by introducing two new consistency measures, and also, show that INTERACT has the important deficiency that it fails for particular types of probability distributions. To fix the deficiency, we propose two new algorithms, which have flexibility of taking advantage of any of the new measures as well as inconsistency rate. Furthermore, through experiments, we compare the three consistency measures, and prove effectiveness of the new algorithms.",
isbn="978-3-642-04595-0"
}

@article{Arauzo2011,
title = "Empirical study of feature selection methods based on individual feature evaluation for classification problems",
journal = "Expert Systems with Applications",
volume = "38",
number = "7",
pages = "8170 - 8177",
year = "2011",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2010.12.160",
url = "http://www.sciencedirect.com/science/article/pii/S095741741001523X",
author = "Antonio Arauzo-Azofra and José Luis Aznarte and José M. Benítez",
keywords = "Feature selection, Feature evaluation, Classification problems, Data reduction, Feature estimation",
abstract = "The use of feature selection can improve accuracy, efficiency, applicability and understandability of a learning process and its resulting model. For this reason, many methods of automatic feature selection have been developed. By using a modularization of feature selection process, this paper evaluates a wide spectrum of these methods. The methods considered are created by combination of different selection criteria and individual feature evaluation modules. These methods are commonly used because of their low running time. After carrying out a thorough empirical study the most interesting methods are identified and some recommendations about which feature selection method should be used under different conditions are provided."
}